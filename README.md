<div align="center">
  <h1>AI Consciousness Research Assistant</h1>
  <p>
    A Retrieval-Augmented Generation (RAG) assistant for exploring, filtering, and chatting with a unified knowledge base on consciousness, built from scholarly papers and expert transcripts.
    <br />
    <a href="#-about-the-project"><strong>Explore the Docs ¬ª</strong></a>
    <br /><br />
    <a href="https://github.com/sidsharmaa/ai-consciousness-project-major">View Demo</a>
    ¬∑
    <a href="https://github.com/sidsharmaa/ai-consciousness-project-major/issues/new?labels=bug&template=bug-report---.md">Report Bug</a>
    ¬∑
    <a href="https://github.com/sidsharmaa/ai-consciousness-project-major/issues/new?labels=enhancement&template=feature-request---.md">Request Feature</a>
  </p>
</div>

---

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#-about-the-project">About The Project</a>
      <ul>
        <li><a href="#key-features">Key Features</a></li>
        <li><a href="#-built-with">Built With</a></li>
      </ul>
    </li>
    <li><a href="#-getting-started">Getting Started</a>
      <ul>
        <li><a href="#-prerequisites">Prerequisites</a></li>
        <li><a href="#-installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#-usage">Usage</a></li>
    <li><a href="#-roadmap">Roadmap</a></li>
    <li><a href="#-contributing">Contributing</a></li>
    <li><a href="#-license">License</a></li>
    <li><a href="#-contact">Contact</a></li>
    <li><a href="#-acknowledgments">Acknowledgments</a></li>
  </ol>
</details>

---

## üèõÔ∏è About The Project

This project provides an end-to-end system for building and querying a specialized knowledge base on the topic of **consciousness**. It moves beyond simple scripting to a professional, modular architecture.

The system automatically downloads academic papers from arXiv, processes local expert transcripts, and unifies them into a single vector store. A command-line interface then allows users to ask complex questions and receive source-aware answers generated by a local Large Language Model (LLM).

### Key Features

* **Config-Driven Pipelines**: All data processing and embedding logic is controlled via a central `config.yaml` file, not hardcoded scripts.
* **Multi-Source Knowledge Base**: Ingests data from both structured sources (arXiv papers) and unstructured text (expert transcripts).
* **Modular & Testable Code**: The core logic is encapsulated in a `QueryBot` class with separated concerns, making it robust and maintainable.
* **Local First**: The entire RAG pipeline runs locally using Ollama, ensuring privacy and control.
* **Source Attribution**: All generated answers are accompanied by citations from the source documents.

### üõ†Ô∏è Built With

* Python 3.11+
* LangChain
* SentenceTransformers
* FAISS
* Ollama
* Pydantic (for configuration)
* Pandas

---

## üöÄ Getting Started

Follow these steps to get a local copy up and running.

### üìã Prerequisites

* **Python >= 3.10**
* **Ollama**: You must have Ollama installed and running. [Download here](https://ollama.com).

### ‚öôÔ∏è Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/sidsharmaa/ai-consciousness-project-major.git](https://github.com/sidsharmaa/ai-consciousness-project-major.git)
    cd ai-consciousness-project-major
    ```
2.  **Create and activate a virtual environment:**
    ```bash
    # For Windows
    python -m venv venv
    .\venv\Scripts\activate

    # For macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Download the LLM:**
    Pull the Mistral model that the chatbot will use.
    ```bash
    ollama pull mistral
    ```

---

## üìñ Usage

The project is run as a sequence of modular, configurable steps.

### 1. (Optional) Configure the Pipeline

All settings for data sources, models, and paths are controlled in `config/config.yaml`. You can modify this file to change keywords, models, or data sources without touching the Python code.

### 2. Build the Knowledge Base

Run the unified embedding pipeline. This single command will process all configured data sources (both arXiv papers and local transcripts) and build the FAISS vector store.

```bash
python -m src.data.build_vector_store
```
### 3. Start the Ollama Server

```bash
ollama run mistral
```

### 4. Chat with the Assistant

```bash
python -m src.rag_pipeline.main_cli
```

---

##  Roadmap

* [x] Establish a professional, modular project structure.
* [x] Create a config-driven, multi-source data pipeline.
* [x] Refactor query logic into an object-oriented QueryBot class.
* [x] Build a unified vector store from arXiv papers and expert transcripts.
* [ ] Implement Unit and Integration Tests (pytest).
* [ ] Develop a FastAPI REST API for the bot.
* [ ] Containerize the full application with Docker.
* [ ] Add a Streamlit dashboard for interactive analysis.
* [ ] Evaluate and improve answer quality with advanced RAG strategies.

---


##  Contact

**Siddhant Sharma** ‚Äî 
GitHub: [@sidsharmaa](https://github.com/sidsharmaa)

Project Link: [https://github.com/sidsharmaa/ai-consciousness-project-major](https://github.com/sidsharmaa/ai-consciousness-project-major)

---

##  Resources used

* LangChain & SentenceTransformers
* Ollama team for lightweight LLMs
* The researchers whose work forms our knowledge base
* [arXiv API](https://arxiv.org/help/api/)

<p align="right">(<a href="#top">back to top</a>)</p>
