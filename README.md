<div align="center">
<h1 id="top">AI Consciousness Research Assistant</h1>
<p>
A Retrieval-Augmented Generation (RAG) assistant for exploring, filtering, and chatting with a unified knowledge base on consciousness, built from scholarly papers and expert transcripts.
<br />
<a href="#-about-the-project"><strong>Explore the Project »</strong></a>
<br /><br />
<a href="https://github.com/sidsharmaa/ai-consciousness-project-major/issues/new?labels=bug&template=bug-report---.md">Report Bug</a>
·
<a href="https://github.com/sidsharmaa/ai-consciousness-project-major/issues/new?labels=enhancement&template=feature-request---.md">Request Feature</a>
</p>
</div>

<details>
<summary>Table of Contents</summary>
<ol>
<li><a href="#-about-the-project">About The Project</a>
<ul>
<li><a href="#key-features">Key Features</a></li>
<li><a href="#-built-with">Built With</a></li>
</ul>
</li>
<li><a href="#-getting-started">Getting Started</a>
<ul>
<li><a href="#-prerequisites">Prerequisites</a></li>
<li><a href="#-installation">Installation</a></li>
</ul>
</li>
<li><a href="#-usage">Usage</a></li>
<li><a href="#-contributing">Contributing</a></li>
<li><a href="#-license">License</a></li>
<li><a href="#-contact">Contact</a></li>
<li><a href="#-acknowledgments">Acknowledgments</a></li>
</ol>
</details>

##  About The Project

This project provides an end-to-end system for building and querying a specialized knowledge base on the topic of consciousness. It moves beyond simple scripting to a professional, modular, and config-driven architecture.

The system automatically processes academic papers from arXiv and local expert transcripts, unifying them into a single, queryable vector store. A command-line interface then allows users to ask complex questions and receive source-aware answers generated by a local Large Language Model (LLM).

### Key Features

* **Config-Driven Pipelines**: All data processing and embedding logic is controlled via a central `config.yaml` file, not hardcoded scripts.
* **Multi-Source Knowledge Base**: Ingests data from both structured sources (arXiv papers) and unstructured text (expert transcripts).
* **Object-Oriented & Modular**: Core logic is encapsulated in a `QueryBot` class with separated concerns for maintainability and testing.
* **Local First**: The entire RAG pipeline runs locally using Ollama, ensuring privacy and control.
* **Source Attribution**: All generated answers are accompanied by citations from the source documents.

###  Built With

* Python 3.11+
* LangChain
* SentenceTransformers
* FAISS-CPU
* Ollama (Mistral)
* Pydantic
* Pandas

##  Getting Started

Follow these steps to get a local copy up and running.

###  Prerequisites

* **Python >= 3.10**
* **Ollama**: You must have Ollama installed and running. [Download here](https://ollama.com).

###  Installation

Clone the repository:

```bash
git clone https://github.com/sidsharmaa/ai-consciousness-project-major.git
cd ai-consciousness-project-major
```

Create and activate a virtual environment:

```bash
# For Windows
python -m venv venv
.\venv\Scripts\activate

# For macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Download the LLM:
Pull the Mistral model that the chatbot will use.

```bash
ollama pull mistral
```

##  Usage

The project is run as a sequence of modular, configurable steps.

### 1. (Optional) Configure the Pipeline

All settings for data sources, models, and paths are controlled in `config/config.yaml`. You can modify this file to change keywords, models, or data sources without touching the Python code.

### 2. Build the Knowledge Base

Run the unified embedding pipeline. This single command will process all configured data sources (both arXiv papers and local transcripts) and build the FAISS vector store.

```bash
python -m src.data.build_vector_store
```

### 3. Run the Chatbot

The chatbot requires two separate terminals to run: one for the Ollama server (the "engine") and one for the CLI application (the "interface").

**Terminal 1:** Start the Ollama Server

```bash
ollama run mistral
```

**Terminal 2:** Start the Chatbot CLI

```bash
python -m src.rag_pipeline.main_cli
```

The application will start, ask you to choose an answer length, and then you can begin asking questions.

##  Contributing

Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag "enhancement".

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

##  License

Distributed under the MIT License. See `LICENSE.txt` for more information.

##  Contact

**Siddhant Sharma** — [@sidsharmaa](https://github.com/sidsharmaa)

Project Link: [https://github.com/sidsharmaa/ai-consciousness-project-major](https://github.com/sidsharmaa/ai-consciousness-project-major)

##  Acknowledgments

* LangChain & SentenceTransformers
* Ollama team for lightweight LLMs
* The researchers whose work forms our knowledge base
* [arXiv API](https://arxiv.org/help/api/)

<p align="right">(<a href="#top">back to top</a>)</p>
