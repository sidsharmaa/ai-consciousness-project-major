Title,Summary,Authors,Published,Updated,PDF_URL,Primary_Category,Categories,text
The Thin Line Between Comprehension and Persuasion in LLMs,"Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.","['Adrian de Wynter', 'Tangming Yuan']",2025-07-02 17:46:56+00:00,2025-07-02 17:46:56+00:00,http://arxiv.org/pdf/2507.01936v1,cs.CL,"['cs.CL', 'cs.CY']","the thin line between comprehension and persuasion in llms large language models (llms) are excellent at maintaining high-level,
convincing dialogues. they are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. this,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of llms and their comprehension of dialogue. in this work we
begin by evaluating llms' ability to maintain a debate--one of the purest yet
most complex forms of human communication. then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. we find that
llms are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. we also note that awareness or
suspicion of ai involvement encourage people to be more critical of the
arguments made. when polling llms on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. our findings
tie the shortcomings of llms-as-evaluators to their (in)ability to understand
the context. more broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. hence, the modelling of pragmatic context and
coherence are secondary to effectiveness."
Probing Evaluation Awareness of Language Models,"Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.","['Jord Nguyen', 'Khiem Hoang', 'Carlo Leonardo Attubato', 'Felix Hofst√§tter']",2025-07-02 15:12:43+00:00,2025-07-02 15:12:43+00:00,http://arxiv.org/pdf/2507.01786v1,cs.CL,"['cs.CL', 'cs.AI']","probing evaluation awareness of language models language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. this has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to ai governance frameworks and voluntary industry
commitments. in this paper, we study evaluation awareness in
llama-3.3-70b-instruct. we show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. we also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. more
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception."
